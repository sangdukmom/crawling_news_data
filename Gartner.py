import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# ê°€íŠ¸ë„ˆ ê²€ìƒ‰ URL
BASE_URL = "https://www.gartner.com/en/search?keyword="

# ê²€ìƒ‰í•  í‚¤ì›Œë“œ
KEYWORD = "TSMC"

# ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
articles = []

def crawl_gartner_news(keyword):
    """Gartnerì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë‰´ìŠ¤ ì œëª©, ë§í¬, ë‚ ì§œ, ë³¸ë¬¸ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # í‚¤ì›Œë“œ ê²€ìƒ‰ URL
        search_url = f"{BASE_URL}{keyword}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìš”ì²­
        response = requests.get(search_url, headers=headers, timeout=10)
        if response.status_code != 200:
            print(f"âš ï¸ {search_url} ì ‘ì† ì‹¤íŒ¨ (status: {response.status_code})")
            return

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, "html.parser")

        # ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ íƒœê·¸ ìˆ˜ì • í•„ìš”)
        news_list = soup.find_all("div", class_="search-results__item")  # ê°€íŠ¸ë„ˆ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ íƒœê·¸

        for news in news_list:
            try:
                title_tag = news.find("a", class_="search-results__title-link")
                title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
                link = title_tag["href"] if title_tag else None

                date_tag = news.find("span", class_="search-results__date")
                date = date_tag.get_text(strip=True) if date_tag else "ë‚ ì§œ ì—†ìŒ"

                # ìƒì„¸ ë³¸ë¬¸ í¬ë¡¤ë§ (ë‰´ìŠ¤ ë§í¬ë¡œ ì ‘ì†)
                article_content = ""
                if link:
                    full_link = f"https://www.gartner.com{link}" if link.startswith("/") else link
                    article_response = requests.get(full_link, headers=headers, timeout=10)
                    if article_response.status_code == 200:
                        article_soup = BeautifulSoup(article_response.text, "html.parser")
                        content_tag = article_soup.find("div", class_="article-content")  # ë³¸ë¬¸ íƒœê·¸ (ìˆ˜ì • í•„ìš”í•  ìˆ˜ë„ ìˆìŒ)
                        article_content = content_tag.get_text(strip=True) if content_tag else "ë³¸ë¬¸ ì—†ìŒ"

                # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                articles.append({
                    "keyword": keyword,
                    "title": title,
                    "link": full_link,
                    "date": date,
                    "content": article_content
                })
                time.sleep(1)  # í¬ë¡¤ë§ ì†ë„ ì¡°ì ˆ

            except Exception as e:
                print(f"âŒ ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

# í¬ë¡¤ë§ ì‹¤í–‰
print(f"ğŸ” Gartnerì—ì„œ '{KEYWORD}' í‚¤ì›Œë“œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
crawl_gartner_news(KEYWORD)

# ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë° ì €ì¥
df = pd.DataFrame(articles)
df.to_csv("gartner_news.csv", index=False, encoding="utf-8-sig")

print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! gartner_news.csv ì €ì¥ë¨")
