import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
URLS = [
    "https://www.gartner.com/en/newsroom",
    "https://www.techinsights.com/resources/blogs",
    "https://omdia.tech.informa.com/pr",
    "https://semi.org",
    "https://www.trendforce.com/presscenter",
    "https://digitimes.com.tw",
    "https://digitimes.com",
    "https://www.thelec.kr/",
    "https://www.reuters.com/",
    "https://www.bloomberg.com/asia",
    "https://biz.chosun.com/",
    "https://www.techdaily.co.kr/",
    "https://www.idc.com/resource-center/press-releases",
    "https://www.counterpointresearch.com/",
    "https://zdnet.co.kr/",
    "https://www.trendforce.com/presscenter",
    "https://semiengineering.com",
    "https://eetimes.com",
    "https://epnc.co.kr",
    "https://icsmart.cn/",
    "https://pc.watch.impress.co.jp",
    "https://9to5mac.com",
    "https://nikkei.com",
    "https://theverge.com",
    "https://thebell.co.kr",
    "https://donga.com",
    "https://mk.co.kr",
    "https://sankei.com",
    "https://laoyaoba.com",
    "https://yna.co.kr",
    "https://yomiuri.co.jp",
    "https://edaily.co.kr",
    "https://electronicsweekly.com",
    "https://etnews.com",
    "https://yicai.com",
    "https://joongang.co.kr",
    "https://fpdisplay.com",
    "https://hankyung.com",
    "https://imidex.org",
    "https://powerelectronicsnews.com",
    "https://powerelectronicsworld.net",
    "https://compoundsemiconductor.net"
]

# í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
KEYWORDS = [
    "DRAM", "NAND", "HBM", "TSMC", "SMIC", "Globalfoundries",
    "Automotive", "Electric Vehicle", "Smartphone", "PC", "Notebook", "Server", "AI Server",
    "Semiconductor", "Microsoft", "Google", "AWS", "Meta", "CoreWeave", "Nvidia", "openAI", "SiC"
]

# ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
articles = []

def crawl_news(url):
    """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ íƒ€ì´í‹€ê³¼ URLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            print(f"âš ï¸ {url} ì ‘ì† ì‹¤íŒ¨ (status: {response.status_code})")
            return
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°ì‹¸ëŠ” íƒœê·¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        articles_on_page = soup.find_all("a")  # ì¼ë°˜ì ìœ¼ë¡œ ê¸°ì‚¬ ë§í¬ëŠ” <a> íƒœê·¸ì— í¬í•¨ë¨
        
        for article in articles_on_page:
            title = article.get_text(strip=True)
            link = article.get("href")

            # í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ëœ ê²½ìš°ë§Œ ì €ì¥
            if link and any(keyword.lower() in title.lower() for keyword in KEYWORDS):
                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if link.startswith("/"):
                    link = url + link
                articles.append({"site": url, "title": title, "link": link})

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {url} - {e}")

# í¬ë¡¤ë§ ì‹¤í–‰
for url in URLS:
    print(f"ğŸ” {url}ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
    crawl_news(url)
    time.sleep(random.uniform(1, 3))  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ëœë¤ ë”œë ˆì´

# ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë° ì €ì¥
df = pd.DataFrame(articles)
df.to_csv("news_data.csv", index=False, encoding="utf-8-sig")
print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! news_data.csv ì €ì¥ë¨")
